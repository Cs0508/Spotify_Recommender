from pathlib import Path
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from tqdm import tqdm

# å®šä¹‰æ–‡ä»¶è·¯å¾„
data_folder = Path(__file__).parent / "dataset/input"
config_folder = Path(__file__).parent / "dataset/config"

print("\nğŸ“‚ æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„:", data_folder.resolve())

# è¯»å–åˆ—åé…ç½®çš„å‡½æ•°
def read_columns_from_txt(filename):
    file_path = config_folder / filename
    if file_path.exists():
        with open(file_path, "r", encoding="utf-8") as f:
            return [line.strip() for line in f.readlines() if line.strip()]
    return []

# è¯»å–åˆ—åé…ç½®
date_cols = read_columns_from_txt("date_cols.txt")
id_cols = read_columns_from_txt("id_cols.txt")
numeric_cols = read_columns_from_txt("numeric_cols.txt")
ignore_cols = read_columns_from_txt("ignore_cols.txt")

print("\nğŸ“‹ è¯»å–çš„åˆ—åé…ç½®:")
print("ğŸ“† æ—¥æœŸåˆ—:", date_cols)
print("ğŸ”¢ ID åˆ—:", id_cols)
print("ğŸ“Š æ•°å€¼åˆ—:", numeric_cols)
print("ğŸš« å¿½ç•¥åˆ—:", ignore_cols)

# è·å–æ‰€æœ‰ CSV æ–‡ä»¶
csv_files = list(data_folder.glob("*.csv"))

# å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶ï¼Œé€€å‡º
if not csv_files:
    print("âš ï¸ æ²¡æœ‰æ‰¾åˆ° CSV æ–‡ä»¶ï¼è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶å¤¹ã€‚")
    exit()

# æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
print("\nğŸ“‹ æ‰¾åˆ°ä»¥ä¸‹ CSV æ–‡ä»¶:")
for i, file in enumerate(csv_files):
    print(f"{i+1}. {file.name}")

# è®©ç”¨æˆ·é€‰æ‹©è¦å¤„ç†çš„æ–‡ä»¶
selected_indices = input("\nè¯·è¾“å…¥è¦å¤„ç†çš„æ–‡ä»¶ç¼–å·ï¼ˆå¤šä¸ªæ–‡ä»¶è¯·ç”¨é€—å·åˆ†éš”ï¼Œä¾‹å¦‚ 1,3,5ï¼‰ï¼š")

# è§£æç”¨æˆ·è¾“å…¥
try:
    selected_indices = [int(i.strip()) - 1 for i in selected_indices.split(",")]
    selected_files = [csv_files[i] for i in selected_indices if 0 <= i < len(csv_files)]
except ValueError:
    print("âš ï¸ è¾“å…¥æ ¼å¼é”™è¯¯ï¼Œè¯·è¾“å…¥æœ‰æ•ˆçš„æ–‡ä»¶ç¼–å·ï¼")
    exit()

# é¢„å¤„ç†å‡½æ•°
def process_file(file_path):
    # è¯»å– CSVï¼Œä¿ç•™åŸå§‹åˆ—é¡ºåº
    df = pd.read_csv(file_path)
    original_columns = df.columns.tolist()  # è®°å½•åŸå§‹åˆ—é¡ºåº

    # ç¡®å®šæœªåˆ†ç±»çš„åˆ—ï¼ˆåŸæ ·ä¿ç•™ï¼‰
    all_cols = set(df.columns)
    classified_cols = set(date_cols + id_cols + numeric_cols + ignore_cols)
    other_cols = list(all_cols - classified_cols)  # æœªåˆ†ç±»çš„åˆ—ï¼ŒåŸæ ·ä¿ç•™

    print(f"\nğŸ“Œ æœªåˆ†ç±»çš„åˆ—ï¼ˆåŸæ ·ä¿ç•™ï¼‰: {other_cols}")

    # åˆ é™¤ ignore_cols ä¸­çš„åˆ—
    df.drop(columns=ignore_cols, errors='ignore', inplace=True)

    # åªä¿ç•™æœªè¢«å¿½ç•¥çš„åˆ—ï¼Œå¹¶ä¿æŒåŸå§‹é¡ºåº
    remaining_cols = [col for col in original_columns if col in df.columns]  # ä¿ç•™å…¶ä»–åˆ—å¹¶æŒ‰åŸå§‹é¡ºåºæ’åˆ—

    # ä¿è¯æœªåˆ†ç±»çš„åˆ—ï¼ˆother_colsï¼‰ä¹Ÿè¢«ä¿ç•™
    df = df[remaining_cols]  # å°†åˆ—æŒ‰åŸå§‹é¡ºåºé‡æ–°æ’åˆ—ï¼ŒåŒ…å«æœªåˆ†ç±»çš„åˆ—

    # å»é‡
    df.drop_duplicates(inplace=True)

    # å¤„ç†ç¼ºå¤±å€¼ï¼ˆæ•°å€¼åˆ—å¡«å……ä¸­ä½æ•°ï¼‰
    df.fillna(df.median(numeric_only=True), inplace=True)

    # å¤„ç†æ—¥æœŸåˆ—
    for col in date_cols:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors='coerce')
    df.dropna(subset=date_cols, inplace=True)

    # ç¼–ç  ID ç±»åˆ«å˜é‡
    for col in id_cols:
        if col in df.columns:
            df[col] = df[col].astype('category').cat.codes

    # å½’ä¸€åŒ–æ•°å€¼ç‰¹å¾
    if numeric_cols:
        scaler = MinMaxScaler()
        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

    return df

# å¤„ç†æ‰€é€‰çš„ CSV æ–‡ä»¶
for file in tqdm(selected_files, desc="å¤„ç†ä¸­"):
    df_cleaned = process_file(file)
    cleaned_file_path = data_folder.parent / f"output/cleaned_{file.name}"

    print("\nğŸ“ é¢„è§ˆå¤„ç†åçš„æ•°æ®ï¼ˆå‰ 5 è¡Œï¼‰ï¼š")
    print(df_cleaned.head())

    df_cleaned.to_csv(cleaned_file_path, index=False)
    print(f"âœ… å·²å¤„ç†å¹¶ä¿å­˜: {cleaned_file_path}")
